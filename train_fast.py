#!/usr/bin/env python3
"""
Fast training script with optimizations for smaller datasets and rapid experimentation.
Supports multiple dataset sizes and includes performance optimizations.
"""

import os
import sys
import argparse
import time
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

import torch
import torch.nn as nn
import numpy as np
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from sklearn.metrics import mean_absolute_error, r2_score
from tqdm import tqdm
import matplotlib.pyplot as plt

from config_quick import get_config
from model import QuantumGNNWithAttention
from data_loader import create_data_loaders

class FastTrainer:
    def __init__(self, config, verbose=True):
        self.config = config
        self.device = config.device
        self.verbose = verbose
        
        # Create directories
        os.makedirs(config.log_dir, exist_ok=True)
        os.makedirs(config.checkpoint_dir, exist_ok=True)
        
        # Load data
        if self.verbose:
            print("Loading data...")
        start_time = time.time()
        self.train_loader, self.val_loader, self.test_loader = create_data_loaders(config)
        load_time = time.time() - start_time
        
        if self.verbose:
            print(f"Data loaded in {load_time:.2f}s")
            print(f"Train batches: {len(self.train_loader)}")
            print(f"Val batches: {len(self.val_loader)}")
            print(f"Test batches: {len(self.test_loader)}")
        
        # Initialize model
        self.model = self.create_model()\n        self.model.to(self.device)\n        \n        # Print model info\n        if self.verbose:\n            total_params = sum(p.numel() for p in self.model.parameters())\n            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n            print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n        \n        # Initialize optimizer\n        self.optimizer = AdamW(\n            self.model.parameters(),\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay\n        )\n        \n        # Scheduler with warmup\n        self.scheduler = CosineAnnealingWarmRestarts(\n            self.optimizer,\n            T_0=max(10, config.num_epochs // 4),\n            T_mult=2,\n            eta_min=config.learning_rate * 0.01\n        )\n        \n        # Loss function with reduction for stability\n        self.criterion = nn.MSELoss()\n        \n        # Metrics tracking\n        self.metrics = {\n            'train_losses': [],\n            'val_losses': [],\n            'train_times': [],\n            'val_times': []\n        }\n        self.best_val_loss = float('inf')\n        self.best_epoch = 0\n    \n    def create_model(self):\n        \"\"\"Create model with automatic input dimension detection\"\"\"\n        # Get sample batch for dimensions\n        sample_batch = next(iter(self.train_loader))\n        input_dim = sample_batch.x.shape[1]\n        edge_dim = sample_batch.edge_attr.shape[1] if sample_batch.edge_attr is not None else 4\n        output_dim = len(self.config.target_properties)\n        \n        if self.verbose:\n            print(f\"Model dimensions: input={input_dim}, edge={edge_dim}, output={output_dim}\")\n        \n        model = QuantumGNNWithAttention(\n            input_dim=input_dim,\n            hidden_dim=self.config.hidden_dim,\n            output_dim=output_dim,\n            num_layers=self.config.num_layers,\n            num_heads=self.config.num_heads,\n            dropout=self.config.dropout,\n            edge_dim=edge_dim\n        )\n        \n        return model\n    \n    def train_epoch(self, epoch):\n        \"\"\"Optimized training epoch\"\"\"\n        self.model.train()\n        epoch_loss = 0\n        num_batches = 0\n        start_time = time.time()\n        \n        # Use tqdm only for verbose mode\n        iterator = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\") if self.verbose else self.train_loader\n        \n        for batch in iterator:\n            batch = batch.to(self.device, non_blocking=True)\n            \n            # Skip invalid batches\n            if batch.y is None or batch.x.shape[0] == 0:\n                continue\n            \n            self.optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            # Forward pass\n            pred = self.model(batch)\n            \n            # Prepare targets\n            batch_size = len(torch.unique(batch.batch))\n            num_properties = len(self.config.target_properties)\n            targets = batch.y.view(batch_size, num_properties)\n            \n            # Loss calculation\n            loss = self.criterion(pred, targets)\n            \n            # Backward pass with gradient clipping\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            \n            epoch_loss += loss.item()\n            num_batches += 1\n            \n            if self.verbose and hasattr(iterator, 'set_postfix'):\n                iterator.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_loss = epoch_loss / max(num_batches, 1)\n        epoch_time = time.time() - start_time\n        \n        self.metrics['train_losses'].append(avg_loss)\n        self.metrics['train_times'].append(epoch_time)\n        \n        return avg_loss, epoch_time\n    \n    def validate(self, epoch):\n        \"\"\"Fast validation\"\"\"\n        self.model.eval()\n        val_loss = 0\n        num_batches = 0\n        start_time = time.time()\n        \n        with torch.no_grad():\n            for batch in self.val_loader:\n                batch = batch.to(self.device, non_blocking=True)\n                \n                if batch.y is None or batch.x.shape[0] == 0:\n                    continue\n                \n                pred = self.model(batch)\n                \n                batch_size = len(torch.unique(batch.batch))\n                num_properties = len(self.config.target_properties)\n                targets = batch.y.view(batch_size, num_properties)\n                \n                loss = self.criterion(pred, targets)\n                val_loss += loss.item()\n                num_batches += 1\n        \n        avg_loss = val_loss / max(num_batches, 1)\n        val_time = time.time() - start_time\n        \n        self.metrics['val_losses'].append(avg_loss)\n        self.metrics['val_times'].append(val_time)\n        \n        return avg_loss, val_time\n    \n    def save_checkpoint(self, epoch, is_best=False):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_val_loss': self.best_val_loss,\n            'metrics': self.metrics,\n            'config': vars(self.config)\n        }\n        \n        if is_best:\n            best_path = Path(self.config.checkpoint_dir) / 'best_model.pt'\n            torch.save(checkpoint, best_path)\n            if self.verbose:\n                print(f\"Saved best model: {best_path}\")\n    \n    def train(self):\n        \"\"\"Main training loop with timing\"\"\"\n        if self.verbose:\n            print(f\"Starting training on {self.device}\")\n            print(f\"Dataset: {self.config.data_dir}\")\n            print(f\"Epochs: {self.config.num_epochs}\")\n        \n        total_start_time = time.time()\n        \n        for epoch in range(self.config.num_epochs):\n            # Training\n            train_loss, train_time = self.train_epoch(epoch)\n            \n            # Validation\n            val_loss, val_time = self.validate(epoch)\n            \n            # Scheduler step\n            self.scheduler.step()\n            \n            # Check for best model\n            is_best = val_loss < self.best_val_loss\n            if is_best:\n                self.best_val_loss = val_loss\n                self.best_epoch = epoch\n                self.save_checkpoint(epoch, is_best=True)\n            \n            # Print progress\n            if self.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                print(f\"Epoch {epoch+1:3d}/{self.config.num_epochs} | \"\n                      f\"Train: {train_loss:.4f} ({train_time:.1f}s) | \"\n                      f\"Val: {val_loss:.4f} ({val_time:.1f}s) | \"\n                      f\"LR: {lr:.2e} | \"\n                      f\"Best: {self.best_val_loss:.4f}\")\n            \n            # Early stopping for tiny datasets\n            if hasattr(self.config, 'data_dir') and 'tiny' in self.config.data_dir:\n                if epoch - self.best_epoch > 10:  # Early stopping\n                    if self.verbose:\n                        print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        \n        total_time = time.time() - total_start_time\n        \n        if self.verbose:\n            print(f\"\\nTraining completed in {total_time:.1f}s\")\n            print(f\"Best validation loss: {self.best_val_loss:.4f} (epoch {self.best_epoch+1})\")\n            avg_epoch_time = total_time / (epoch + 1)\n            print(f\"Average time per epoch: {avg_epoch_time:.1f}s\")\n        \n        return self.metrics\n    \n    def quick_test(self):\n        \"\"\"Quick test on a few batches\"\"\"\n        if not Path(self.config.checkpoint_dir).exists():\n            print(\"No checkpoint found, using current model\")\n        else:\n            best_model_path = Path(self.config.checkpoint_dir) / 'best_model.pt'\n            if best_model_path.exists():\n                checkpoint = torch.load(best_model_path, map_location=self.device)\n                self.model.load_state_dict(checkpoint['model_state_dict'])\n                if self.verbose:\n                    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n        \n        self.model.eval()\n        test_loss = 0\n        num_batches = 0\n        \n        with torch.no_grad():\n            for i, batch in enumerate(self.test_loader):\n                if i >= 5:  # Quick test on first 5 batches\n                    break\n                    \n                batch = batch.to(self.device)\n                if batch.y is None:\n                    continue\n                \n                pred = self.model(batch)\n                batch_size = len(torch.unique(batch.batch))\n                targets = batch.y.view(batch_size, len(self.config.target_properties))\n                \n                loss = self.criterion(pred, targets)\n                test_loss += loss.item()\n                num_batches += 1\n        \n        if num_batches > 0:\n            avg_test_loss = test_loss / num_batches\n            if self.verbose:\n                print(f\"Quick test loss (5 batches): {avg_test_loss:.4f}\")\n            return avg_test_loss\n        else:\n            if self.verbose:\n                print(\"No valid test batches found\")\n            return float('inf')\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Fast quantum GNN training\")\n    parser.add_argument(\"--size\", choices=['tiny', 'small', 'medium', 'full'], \n                       default='small', help=\"Dataset size configuration\")\n    parser.add_argument(\"--epochs\", type=int, help=\"Override number of epochs\")\n    parser.add_argument(\"--batch-size\", type=int, help=\"Override batch size\")\n    parser.add_argument(\"--quiet\", action='store_true', help=\"Reduce output verbosity\")\n    parser.add_argument(\"--test-only\", action='store_true', help=\"Only run quick test\")\n    \n    args = parser.parse_args()\n    \n    # Get configuration\n    config = get_config(args.size)\n    \n    # Override config parameters\n    if args.epochs:\n        config.num_epochs = args.epochs\n    if args.batch_size:\n        config.batch_size = args.batch_size\n    \n    # Create trainer\n    trainer = FastTrainer(config, verbose=not args.quiet)\n    \n    if args.test_only:\n        trainer.quick_test()\n    else:\n        # Train\n        metrics = trainer.train()\n        \n        # Quick test\n        trainer.quick_test()\n        \n        if not args.quiet:\n            print(\"\\nTraining summary:\")\n            print(f\"Final train loss: {metrics['train_losses'][-1]:.4f}\")\n            print(f\"Final val loss: {metrics['val_losses'][-1]:.4f}\")\n            print(f\"Best val loss: {min(metrics['val_losses']):.4f}\")\n            print(f\"Total train time: {sum(metrics['train_times']):.1f}s\")\n\nif __name__ == \"__main__\":\n    main()